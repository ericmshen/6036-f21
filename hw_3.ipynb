{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 6.036 Fall 2021 HW03.ipynb","provenance":[{"file_id":"1qVATXmKXEHNmILDNBSyjWdnbRsUh9sg4","timestamp":1632609723943},{"file_id":"1Rz1GXpwemwD2T3RglkcaIv0E17F4szQv","timestamp":1631537355494},{"file_id":"1T6E4WGHpiX0E86K5sSVD948Wifv037bd","timestamp":1631410788148},{"file_id":"1stG431FnGep6mKCxZ3fUUuRFK9j0Wfz-","timestamp":1613074673710},{"file_id":"1s4MColPrRQTwL0zjRKk60Qv4ruT4pvhI","timestamp":1600566749186},{"file_id":"1b5eDcIaYAGsCqSpzakGqcDw1khc3QicK","timestamp":1582068426989}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Wmv3jlgr4_Ji"},"source":["# MIT 6.036 Spring 2021: Homework 3\n"]},{"cell_type":"markdown","metadata":{"id":"OY6MBM3vsUJV"},"source":["**Setup**\n","\n","**Make a copy of this colab in your own Drive folder before executing. You can do this by going the File > Save a copy in Drive.**\n","\n","Download the code distribution for this homework that contains test cases and helper functions. Run the next code block to download and import the code for this lab."]},{"cell_type":"code","metadata":{"id":"N622h8-D5i-M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632609767605,"user_tz":240,"elapsed":2220,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"9dbfb98a-ce30-4325-9677-67b6dd216623"},"source":["!rm -rf code_and_data_for_hw3*\n","!rm -rf mnist\n","!wget --no-check-certificate --quiet https://go.odl.mit.edu/subject/6.036/_static/catsoop/homework/hw03/code_and_data_for_hw03.zip\n","!unzip code_and_data_for_hw03.zip\n","!mv code_and_data_for_hw03/* .\n","\n","from code_for_hw03 import *\n","import numpy as np\n","from sklearn.datasets import load_boston\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  code_and_data_for_hw03.zip\n","   creating: code_and_data_for_hw03/\n","  inflating: __MACOSX/._code_and_data_for_hw03  \n","  inflating: code_and_data_for_hw03/code_for_hw03.py  \n","  inflating: __MACOSX/code_and_data_for_hw03/._code_for_hw03.py  \n","Importing code_for_hw03\n"]}]},{"cell_type":"markdown","metadata":{"id":"jUS51a8m5rEI"},"source":["## 3) Implementing gradient descent\n","In this section we will implement generic versions of gradient descent and apply these to the logistic regression objective.\n","\n","<b>Note: </b> If you need a refresher on gradient descent,\n","you may want to reference\n","<a href=\"https://canvas.mit.edu/courses/11118/files/1660165?module_item_id=404498\">this week's notes</a>.\n","\n","### 3.1) Implementing Gradient Descent\n","We want to find the $x$ that minimizes the value of the *objective\n","function* $f(x)$, for an arbitrary scalar function $f$.  The function\n","$f$ will be implemented as a Python function of one argument, that\n","will be a numpy column vector.  For efficiency, we will work with\n","Python functions that return not just the value of $f$ at $f(x)$ but\n","also return the gradient vector at $x$, that is, $\\nabla_x f(x)$.\n","\n","We will now implement a generic gradient descent function, `gd`, that\n","has the following input arguments:\n","\n","* `f`: a function whose input is an `x`, a column vector, and\n","  returns a scalar.\n","* `df`: a function whose input is an `x`, a column vector, and\n","  returns a column vector representing the gradient of `f` at `x`.\n","* `x0`: an initial value of $x$, `x0`, which is a column vector.\n","* `step_size_fn`: a function that is given the iteration index (an\n","  integer) and returns a step size.\n","* `num_steps`: the number of iterations to perform\n","\n","Our function `gd` returns a tuple:\n","\n","* x: the value at the final step\n","* fx: the value of f(x) at the final step\n","\n","**Hint:** This is a short function!"]},{"cell_type":"markdown","metadata":{"id":"s03NFuxG6kvt"},"source":["The main function to implement is `gd`, defined below."]},{"cell_type":"code","metadata":{"id":"mNsLE3bg6jt9","executionInfo":{"status":"ok","timestamp":1632609843867,"user_tz":240,"elapsed":292,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}}},"source":["def gd(f, df, x0, step_size_fn, num_steps):\n","    x = x0\n","    for i in range(num_steps):\n","        x -= step_size_fn(i) * df(x)\n","    return x, f(x)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jXu60n-H5_Hz"},"source":["To evaluate results, we also use a simple `package_ans` function,\n","which checks the final `x` and `fx` values."]},{"cell_type":"markdown","metadata":{"id":"aN_XbacQ6Rue"},"source":["The test cases are provided below, but you should feel free (and are encouraged!) to write more of your own."]},{"cell_type":"code","metadata":{"id":"GJcClaqN4nE6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632609847921,"user_tz":240,"elapsed":288,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"f5d01a5e-be23-431f-c31f-af0f6dc124cb"},"source":["test_gd(gd)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Test 1:\n","Passed!\n","Test 2:\n","Passed!\n"]}]},{"cell_type":"markdown","metadata":{"id":"pbuSt5hY645k"},"source":["### 3.2) Numerical Gradient\n","Getting the analytic gradient correct for complicated functions is\n","tricky.  A very handy method of verifying the analytic gradient or\n","even substituting for it is to estimate the gradient at a point by\n","means of *finite differences*.\n","\n","Assume that we are given a function $f(x)$ that takes a column vector\n","as its argument and returns a scalar value.  In gradient descent, we\n","will want to estimate the gradient of $f$ at a particular $x_0.$\n","\n","The $i^{th}$ component of $\\nabla_x f(x_0)$ can be estimated as\n","$$\\frac{f(x_0+\\delta^{i}) - f(x_0-\\delta^{i})}{2\\delta}$$\n","where $\\delta^{i}$ is a column vector whose $i^{th}$ coordinate is\n","$\\delta$, a small constant such as 0.001, and whose other components\n","are zero.\n","Note that adding or subtracting $\\delta^{i}$ is the same as\n","incrementing or decrementing the $i^{th}$ component of $x_0$ by\n","$\\delta$, leaving the other components of $x_0$ unchanged.  Using\n","these results, we can estimate the $i^{th}$ component of the gradient.\n","\n","\n","**For example**, take $x^(0) = (1,2,3)^T$. The gradient $\\nabla_x f(x)$ is a vector of the derivatives of $f(x)$ with respect to each component of $x$, or $\\nabla_x f(x) = (\\frac{df(x)}{dx_1},\\frac{df(x)}{dx_2},\\frac{df(x)}{dx_3})^T$.\n","\n","We can approximate the first component of $\\nabla_x f(x)$ as\n","$$\\frac{f((1,2,3)^T+(0.01,0,0)^T) - f((1,2,3)^T-(0.01,0,0)^T)}{2\\cdot 0.01}.$$\n","\n","(We add the transpose so that these are column vectors.)\n","**This process should be done for each dimension independently,\n","and together the results of each computation are compiled to give the\n","estimated gradient, which is $d$ dimensional.**\n","\n","Implement this as a function `make_num_grad_fn` that takes as arguments the\n","objective function `f` and a value of `delta`, and returns a new\n","**function** that takes an `x` (a column vector of parameters) and\n","returns a gradient column vector.\n","\n","**Note:** Watch  out for aliasing. If you do temp_x = x where x is a vector (numpy array), then temp_x is just another name for the same vector as x and changing an entry in one will change an entry in the other. You should either use x.copy() or remember to change entries back after modification."]},{"cell_type":"code","metadata":{"id":"WPVwGZ-l6XvW","executionInfo":{"status":"ok","timestamp":1632609860072,"user_tz":240,"elapsed":293,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}}},"source":["def make_num_grad_fn(f, delta=0.001):\n","    def df(x):\n","        d = np.zeros(x.shape)\n","        for i in range(x.shape[0]):\n","            smaller = x.copy()\n","            larger = x.copy()\n","            smaller[i] -= delta\n","            larger[i] += delta\n","            d[i] = (1 / (2 * delta)) * (f(larger) - f(smaller))\n","        return d\n","    return df"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kElTR0bL7cbG"},"source":["The test cases are shown below; these use the functions defined in the previous exercise.\n"]},{"cell_type":"code","metadata":{"id":"iiWOdSl_6yAE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632609862255,"user_tz":240,"elapsed":283,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"9d77faf9-c6e0-46cb-905a-b587b2ff0d3a"},"source":["test_num_grad(make_num_grad_fn)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Test 1\n","Passed\n","Test 2\n","Passed\n","Test 3\n","Passed\n","Test 4\n","Passed\n"]}]},{"cell_type":"markdown","metadata":{"id":"WASaSsYu75sG"},"source":["A faster (one function evaluation per entry), though sometimes less\n","accurate, estimate is to use:\n","$$\\frac{f(x_0+\\delta^{i}) - f(x_0)}{\\delta}$$\n","for the $i^{th}$ component of $\\nabla_x f(x_0).$"]},{"cell_type":"markdown","metadata":{"id":"E31sdqyG78jD"},"source":["3.3) Using the Numerical Gradient\n","Recall that our generic gradient descent function takes both a function\n","`f` that returns the value of our function at a given point, and `df`,\n","a function that returns a gradient at a given point.  Write a function\n","`minimize` that takes only a function `f` and uses this function and\n","numerical gradient descent to return the local minimum.  \n","You may use the default of `delta=0.001` for `make_num_grad_fn`.\n","\n","**Hint:** Your definition of `minimize` should call `make_num_grad_fn` exactly\n","once to return a function. Then you may call this function many times in your updates for numerical gradient descent.\n","You should return the same outputs as `gd`."]},{"cell_type":"code","metadata":{"id":"CStwqDem76Bx","executionInfo":{"status":"ok","timestamp":1632609878237,"user_tz":240,"elapsed":280,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}}},"source":["def minimize(f, x0, step_size_fn, num_steps):\n","    \"\"\"\n","    Parameters:\n","      See definitions in part 1\n","    Returns:\n","      same output as gd\n","    \"\"\"\n","    df = make_num_grad_fn(f)\n","    return gd(f, df, x0, step_size_fn, num_steps)\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4gl0FTby8EQq"},"source":["The test cases are below."]},{"cell_type":"code","metadata":{"id":"UxBLWJFm8DnV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632609880063,"user_tz":240,"elapsed":482,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"00d25f2a-f9ba-4e2c-9c8f-e8cb6c3f6402"},"source":["test_minimize(minimize)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Test 1\n","Passed\n","Test 2\n","Passed\n"]}]},{"cell_type":"markdown","metadata":{"id":"zVEt8pQjN6oe"},"source":["### 4) Stochastic gradient"]},{"cell_type":"markdown","metadata":{"id":"PCs20on2OHbj"},"source":["We will now write some general python code to implement gradient descent.\n","\n","sgd takes the following as input: (Recall that the stochastic part refers to using a randomly selected point and corresponding label from the given dataset to perform an update. Therefore, your objective function for a given step will need to take this into account.)\n","\n","<pre>\n","  X: a standard data array (d by n)\n","  y: a standard labels row vector (1 by n)\n","  J: a cost function whose input is a data point (a column vector), a label (1 by 1) and a weight vector w (a column vector) (in that order), and which returns a scalar.\n","  dJ: a cost function gradient (corresponding to J) whose input is a data point (a column vector), a label (1 by 1) and a weight vector w (a column vector) (also in that order), and which returns a column vector.\n","  w0: an initial value of weight vector \n","  step_size_fn: a function that is given the (zero-indexed) iteration index (an integer) and returns a step size.\n","  max_iter: the number of iterations to perform\n","</pre>\n","\n","It returns a tuple:\n","\n","<pre>\n","w: the value of the weight vector at the final step\n","fs: the list of values of J found during all the iterations\n","ws: the list of values of intermediate \n","</pre>\n","\n","**Helpful Note**: We recommend that in your implementation, you append the current value of w to **ws** *before* updating it. Similarly, use the current value to compute the corresponding value of the objective and append it to **fs**. Specifically, the first element of fs should be the value of J calculated with w0, and fs should have length max_iter; similarly, the first element of ws should be w0, and ws should have length max_iter. w is the final w updated max_iter iterations.\n","\n","You might find the function np.random.randint(n) useful in your implementation.\n","\n","Hint: This is a short function; our implementation is around 10 lines."]},{"cell_type":"code","metadata":{"id":"7wOZHgq9Oug-","executionInfo":{"status":"ok","timestamp":1632609898199,"user_tz":240,"elapsed":270,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}}},"source":["def sgd(X, y, J, dJ, w0, step_size_fn, max_iter):\n","    ws, fs, w = [], [], w0.copy()\n","    n = y.shape[1]\n","    np.random.seed(0)\n","    for i in range(max_iter):\n","        k = np.random.randint(n)\n","        xi = X[:,k:k+1]\n","        yi = y[0,k:k+1]\n","        ws.append(w.copy())\n","        fs.append(J(xi, yi, w))\n","        w -= step_size_fn(i) * dJ(xi, yi, w)\n","        if i == max_iter - 1:\n","            return w, fs, ws"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"axrM8_wsO1AZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632609900585,"user_tz":240,"elapsed":578,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"5154862b-7979-4a58-8f46-a21fbc505978"},"source":["# you must have the num_grad function implemented before you can test the sgd function in colab \n","test_sgd(sgd, make_num_grad_fn)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["running test 1\n","running test 2\n","all tests passed\n"]}]},{"cell_type":"markdown","metadata":{"id":"3o9cbeNYPbaA"},"source":["### 5) Tying Everything Together"]},{"cell_type":"markdown","metadata":{"id":"1QlcXc72agXq"},"source":["In the next subsections, we assume that $X$ is a $d \\times n$ matrix, $Y$ is a $1 \\times n$ matrix, and $\\theta$ is a $d \\times 1$ matrix. Rewriting the ridge objective as matrix operations, we find that: \n","\n","$$ J_{\\text{ridge}}(\\theta) = \\frac{1}{n} (\\theta^T X - Y) (\\theta^T X - Y)^T + \\lambda ||\\theta||^2 $$\n","\n","When implementing `objective_func` and `objective_func_grad`, you *do not* need to concatenate a row of ones to `X`. Assume that the `X` input has already been preprocessed as such. "]},{"cell_type":"markdown","metadata":{"id":"XfN-2UC_cKNw"},"source":["#### 5.1 Gradient Descent and Stochastic Gradient Descent for Linear Regression\n"]},{"cell_type":"markdown","metadata":{"id":"tiZDh02Pced7"},"source":["Write a function for $J_{\\text{ridge}}(\\theta)$."]},{"cell_type":"code","metadata":{"id":"3o_ggpY_PXgz","executionInfo":{"status":"ok","timestamp":1632609922308,"user_tz":240,"elapsed":263,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}}},"source":["def objective_func(X, Y, lam):\n","    \"\"\"\n","    inputs: \n","      X: a (dxn) numpy array. \n","      Y: a (1xn) numpy array \n","      lambda: regularization parameter \n","    outputs: \n","      f : a function that takes in a (dx1) numpy array \"theta\" and returns *as a float* the value of the ridge \n","      regression objective when theta=\"theta\"\n","    \"\"\"\n","    def f(theta): \n","        e = theta.T@X - Y\n","        return float((1/X.shape[1]) * e@e.T + lam * theta.T@theta)\n","    return f \n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxjDqCv5-KOF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632609923984,"user_tz":240,"elapsed":265,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"433235bd-6075-47de-f299-0a60a175f169"},"source":["test_obj_func(objective_func)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["All tests passed!\n"]}]},{"cell_type":"markdown","metadata":{"id":"zL9FcOX1cV9E"},"source":["Write a function for $\\nabla J_{\\text{ridge}}(\\theta)$ with respect to $\\theta$. \n"]},{"cell_type":"code","metadata":{"id":"XGjIWe1vPobO","executionInfo":{"status":"ok","timestamp":1632609933143,"user_tz":240,"elapsed":266,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}}},"source":["def objective_func_grad(X, Y, lam):\n","    \"\"\"\n","    inputs: \n","      X: a (dxn) numpy array. \n","      Y: a (1xn) numpy array \n","      lambda: regularization parameter \n","    outputs: \n","      df : a function that takes in a (dx1) numpy array \"theta\" and returns the gradient of the ridge regression \n","      objective when theta=\"theta\" \n","    \"\"\"\n","    def df(theta): \n","        return (2/X.shape[1]) * X@(theta.T@X-Y).T + 2 * lam * theta\n","    return df "],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"zyP5Evkg-NIV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632609935016,"user_tz":240,"elapsed":260,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"a50c6fa8-8e54-408f-c6ec-e25b3cbcbb6b"},"source":["test_d_obj_func(objective_func_grad)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["all tests passed!\n"]}]},{"cell_type":"markdown","metadata":{"id":"icX8FISmPozV"},"source":["#### 5.2 Finding the Best Parameters\n"]},{"cell_type":"markdown","metadata":{"id":"ymWFpDbDqsbR"},"source":["Let's load the Boston Housing dataset. Our goal is to build a linear regression model (with regularization) to predict the TARGET_VAL (which is the median value of owner-occupied homes) using all other available features in the dataset.\n","\n","For more information about the Boston housing dataset, please visit this [link](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). \n","\n","Note that the data pre-processing routine below normalizes each feature. You will learn more about Feature transformations in Week 5.\n","\n","In what follows, we use Cross-Validation to select the best hyperparamters for gradient descent on the ridge regression model. Using the best hyperparameters, we will then make predictions on a reserved test set. You will also compare the results when using the gradient descent based implementation vs the analytic (closed form) solution."]},{"cell_type":"code","metadata":{"id":"A1Mwg5Bnp67n","executionInfo":{"status":"ok","timestamp":1632609940454,"user_tz":240,"elapsed":280,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}}},"source":["## DO NOT EDIT BELOW.\n","# Pre-Processing the data and returning the train and test sets.\n","\n","# load the dataset and do some data exploration\n","X_raw, y_raw = load_boston(return_X_y=True)\n","raw_data = np.concatenate((X_raw, y_raw[:, None]), axis=1)\n","xvars = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n","yvars = [\"TARGET_VAL\"]\n","\n","data = pd.DataFrame(raw_data, columns=xvars+yvars)\n","\n","# Get the train and test splits to be used later.\n","X_train, y_train, X_test, y_test = get_data_splits_with_transforms(data, xvars, yvars)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"55Z4plUevNrU"},"source":["**CODE REQUIRED HERE** Before we start using the Boston Housing dataset, let's implement the `ridge_gd` function. Given an input `X_train`, `y_train`, `lam`, `theta` and `step_size_fn`, run gradient descent on `X_train` and `y_train` starting from `theta = (dx1) vector of zeros`. Return the value of $\\theta$ after running 2000 iterations of gradient descent. \n","\n","```\n","inputs: \n","  X_train: a dxn numpy array \n","  y_train: a 1xn numpy array \n","  lam: lambda \n","  step_size_fn: a function that takes in i, the current training iteration, and returns the step size for iteration i \n","\n","outputs: \n","  theta: value of theta after 2000 iterations of gradient descent \n","```\n","**Hint**: `objective_func` and `objective_func_grad` are very useful here! \\\n","**Hint**: You can also use your `gd` function \\\n","**Hint**: Previously, you've minimized f as a function of x. Now, X and y are constant. What variable are you minimizing over now? \\"]},{"cell_type":"code","metadata":{"id":"T8ikZvenxl_O","executionInfo":{"status":"ok","timestamp":1632610135257,"user_tz":240,"elapsed":294,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}}},"source":["def ridge_gd(X_train, y_train, lam, step_size_fn): \n","  # TODO \n","  # hint: number of iterations = 2000 \n","  # hint: start from theta = (dx1) vector of zeros\n","  theta = np.zeros((X_train.shape[0], 1))\n","  f, df = objective_func(X_train, y_train, lam), objective_func_grad(X_train, y_train, lam)\n","  return gd(f, df, theta, step_size_fn, 2000)[0]"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGfxOAIMvuUK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632610136956,"user_tz":240,"elapsed":285,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"07221788-5676-47de-c91a-67ce65a8e264"},"source":["test_ridge_gd(ridge_gd)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["all tests pass!\n"]}]},{"cell_type":"markdown","metadata":{"id":"fcyFKWP233ir"},"source":["**CODE REQUIRED HERE** In `cross_validate_gd`, run 5-fold cross-validation on the X and y dataset. Use gradient descent to train a linear model for the `X`, `y` data. We've provided a for loop that iterates over each split. In this code: \n","\n","```\n","  X_train_split, y_train_split: data to use for training. This is a (d x n) numpy array, where n=the number of datapoints in k-1 folds \n","  X_val_split, y_val_split: data to use for evaluating the model. This is a (d x n) numpy array, where n=the number of datapoints in 1 fold\n","```\n","\n","**Hint**: Use `ridge_gd` here. \\\n","**Hint**: Take a look at the solutions for last week's cross_validate code if you get stuck"]},{"cell_type":"code","metadata":{"id":"BX70rLBt33it","executionInfo":{"status":"ok","timestamp":1632610398710,"user_tz":240,"elapsed":340,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}}},"source":["def cross_validate_gd(X, y, lam, step_size_fn):\n","  \"\"\"\n","  Returns k-fold cross-validation loss. On each of the k folds, \n","    train a linear regression model using gradient descent. Return \n","    the average loss across the k folds. \n","  \"\"\"\n","  total_loss = 0\n","  kf = KFold(n_splits=5)\n","  for train_index, test_index in kf.split(X, y=y):\n","    X_train_split, y_train_split = X[train_index].T, y[train_index].T\n","    # TODO - train model on X_train_split, y_train_split using gradient descent\n","    # hint - use variables step_size_fn and lam\n","    th = ridge_gd(X_train_split, y_train_split, lam, step_size_fn)\n","    X_val_split, y_val_split = X[test_index].T, y[test_index].T\n","    # TODO - evaluate model on X_val_split, y_val_split, add loss to total_loss\n","    total_loss += objective_func(X_val_split, y_val_split, 0)(th)\n","  return total_loss / kf.n_splits"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HMkgs4rfXWMz"},"source":["Now it's time to run grid search! We are interested in running grid search over $\\lambda \\in \\{{1e-4, 1e-3, \\cdots, 1e-1\\}}$ and $\\eta \\in \\{{1e-6, 1e-5, \\cdots, 1e-2\\}}$. \n","\n","These two cells are ready to run if you've correctly implemented `cross_validate_gd`. Use the outputs of these cells to answer the rest of problem 5.2. \n","\n","We've also already implemented `cross_validate_analytic` for you. This function returns the cross-validation loss for linear regression models trained with the analytic solution for the squared loss equation. \n","\n","**Note: The next two cells print the cross-validation loss, not the testing set loss! Run the last cell in this notebook for the testing set loss.**\n"]},{"cell_type":"code","metadata":{"id":"nttJDNnpc18o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632610423568,"user_tz":240,"elapsed":3441,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"fb2ea807-1011-4dce-b8f9-475f91b41e2b"},"source":["learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n","lams = [1e-1, 1e-2, 1e-3, 1e-4]\n","\n","# This code runs grid search over the training parameters in `learning_rates` and `lams`\n","for rate in learning_rates:\n","  for lam in lams:\n","    learning_rate_fn = lambda i : rate # learning rate = `rate` throughout training\n","    cross_validation_loss = cross_validate_gd(X_train, y_train, lam, learning_rate_fn)\n","    print(f\"Loss on dataset with lambda={lam}, rate={rate} : cross_validation_loss {cross_validation_loss:.6f}\")"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss on dataset with lambda=0.1, rate=0.01 : cross_validation_loss 0.423505\n","Loss on dataset with lambda=0.01, rate=0.01 : cross_validation_loss 0.488424\n","Loss on dataset with lambda=0.001, rate=0.01 : cross_validation_loss 0.504579\n","Loss on dataset with lambda=0.0001, rate=0.01 : cross_validation_loss 0.506418\n","Loss on dataset with lambda=0.1, rate=0.001 : cross_validation_loss 0.398521\n","Loss on dataset with lambda=0.01, rate=0.001 : cross_validation_loss 0.404061\n","Loss on dataset with lambda=0.001, rate=0.001 : cross_validation_loss 0.404962\n","Loss on dataset with lambda=0.0001, rate=0.001 : cross_validation_loss 0.405056\n","Loss on dataset with lambda=0.1, rate=0.0001 : cross_validation_loss 0.522173\n","Loss on dataset with lambda=0.01, rate=0.0001 : cross_validation_loss 0.518517\n","Loss on dataset with lambda=0.001, rate=0.0001 : cross_validation_loss 0.518152\n","Loss on dataset with lambda=0.0001, rate=0.0001 : cross_validation_loss 0.518116\n","Loss on dataset with lambda=0.1, rate=1e-05 : cross_validation_loss 0.910449\n","Loss on dataset with lambda=0.01, rate=1e-05 : cross_validation_loss 0.910231\n","Loss on dataset with lambda=0.001, rate=1e-05 : cross_validation_loss 0.910210\n","Loss on dataset with lambda=0.0001, rate=1e-05 : cross_validation_loss 0.910207\n","Loss on dataset with lambda=0.1, rate=1e-06 : cross_validation_loss 1.030341\n","Loss on dataset with lambda=0.01, rate=1e-06 : cross_validation_loss 1.030339\n","Loss on dataset with lambda=0.001, rate=1e-06 : cross_validation_loss 1.030338\n","Loss on dataset with lambda=0.0001, rate=1e-06 : cross_validation_loss 1.030338\n"]}]},{"cell_type":"code","metadata":{"id":"bDvZz6A4NDBF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632610429853,"user_tz":240,"elapsed":297,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"f6bfa994-9117-442c-f1f9-ff0aa11acf2d"},"source":["lams = [1e-1, 1e-2, 1e-3, 1e-4]\n","\n","# This code runs grid search over the training parameters in `lams`\n","for lam in lams:\n","  cross_validation_loss = cross_validate_analytic(X_train, y_train, lam).item()\n","  print(f\"Loss on dataset with lambda={lam}: cross_validation_loss {cross_validation_loss:.6f}\")"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss on dataset with lambda=0.1: cross_validation_loss 0.551118\n","Loss on dataset with lambda=0.01: cross_validation_loss 0.553890\n","Loss on dataset with lambda=0.001: cross_validation_loss 0.554177\n","Loss on dataset with lambda=0.0001: cross_validation_loss 0.554205\n"]}]},{"cell_type":"markdown","metadata":{"id":"5RKWyzPRkQXl"},"source":["We will now use the best params found above to build a model on the entire training set (X_train, y_train), get the $\\theta$ values and use them to make predictions for the test set (X_test) and evaluate the error using the *actual* values (y_test). We will compare this error for the gradient descent based implementation vs the analytic solution.\n","\n","\n","**CODE REQUIRED HERE**:\n","\n","1. Update **best_lam_gd** and **best_rate_gd** using the best $\\lambda$ and $\\eta$ values you found using **cross_validate_gd**() above.\n","\n","2. Update **best_lam_analytic** using the best $\\lambda$ value found by using **cross_validate_analytic**() above.\n","\n"]},{"cell_type":"code","metadata":{"id":"qhJFMnVUjm-Z","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"ok","timestamp":1632610555102,"user_tz":240,"elapsed":695,"user":{"displayName":"Eric Shen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYMaPx9BydCeqWB25V_UXujlI_OnGcycJcdDFPiFE=s64","userId":"14478004807070179872"}},"outputId":"e3061425-7aa0-4fa9-c5d3-0f14fb394746"},"source":["#### Using the functions above along with the best performing hyperparams to\n","### determine the test set errors. Please specify the best lambda and learning \n","### rates for the GD and Analytic cases that you found above.\n","\n","# GD\n","best_lam_gd = 0.1      ### to be specified\n","best_rate_gd = 0.001  ### to be specified\n","\n","# get_gd_predictions() function is defined in the hw03 code you imported at \n","# the very top. Check the code out if you are curious about the implementation.\n","gd_predictions, gd_error = get_gd_predictions_and_error(\n","    objective_func, objective_func_grad, gd, X_train, y_train, X_test, y_test, best_lam_gd, best_rate_gd)\n","\n","# Analytic\n","best_lam_analytic = 0.1 ### to be specified\n","\n","# get_analytic_predictions_and_error() function is defined in the hw03 code \n","# you imported at the very top. Check the code out if you are curious about the \n","# implementation.\n","analytic_predictions, analytic_error = get_analytic_predictions_and_error(\n","    X_train, y_train, X_test, y_test, best_lam_analytic)\n","\n","\n","print(f\"Test loss for GD based implementation={gd_error:0.3f}\")\n","print(f\"Test loss for Analytic (closed form) implementation={analytic_error:0.3f}\")\n","\n","\n","#### (Optional) Compare the results by viewing the scatter plots for predictions.\n","plt.scatter(y_test, gd_predictions, color='red', label='GD')\n","plt.scatter(y_test, analytic_predictions, color='blue', label='Analytic')\n","plt.xlabel('Actual')\n","plt.ylabel('Predictions')\n","plt.title('Predictions vs Actual Scatter Plot')\n","plt.xlim([-3, 3])\n","plt.ylim([-3, 3])\n","plt.legend(loc=\"upper right\")\n","plt.show()"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss for GD based implementation=0.824\n","Test loss for Analytic (closed form) implementation=0.892\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]}]}